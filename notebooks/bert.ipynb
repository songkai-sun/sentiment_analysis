{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis based on Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\i'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\p'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\@'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\?'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\L'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\['\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\T'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\C'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\I'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\#'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\B'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\A'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\E'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\,'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\('\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\W'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\w'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\)'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\~'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\K'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
      "/var/folders/xt/dp7pk30x62x1q4kkv6hhdvqh0000gn/T/ipykernel_3605/3511970834.py:17: DeprecationWarning: invalid escape sequence '\\j'\n",
      "  t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n"
     ]
    }
   ],
   "source": [
    "with open('posdata.txt') as f:\n",
    "    pos_data = f.readlines()\n",
    "with open('negdata.txt') as f:\n",
    "    neg_data = f.readlines()\n",
    "with open('text.txt', encoding='utf-8') as f:\n",
    "    predict_data = f.readlines()\n",
    "predict_index = []\n",
    "for i, t_data in enumerate(predict_data):\n",
    "    aa = t_data.strip('\\n').strip('?').strip(' ').strip(']').split('\\\\')\n",
    "    for j, a in enumerate(aa):\n",
    "        a = a.strip('?').strip(' ').strip(']')\n",
    "        if a == '':\n",
    "            continue\n",
    "        if a[0] == 'u' and len(a) != 5:\n",
    "            del aa[j]\n",
    "    t_data = '\\\\'.join(aa)\n",
    "    t_data = t_data.replace('\\\\u\\n', '').strip('\\\\').encode(\"utf-8\").decode(\"unicode_escape\").replace('\\n', '').replace('???', '')\n",
    "    t_data_list = t_data.split()\n",
    "    text = ' '.join(t_data_list[1:])\n",
    "    index = t_data_list[0]\n",
    "    predict_index.append(int(index))\n",
    "    predict_data[i] = text\n",
    "pos_label = [1] * len(pos_data)\n",
    "neg_label = [0] * len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pos_data + neg_data\n",
    "all_label = pos_label + neg_label\n",
    "\n",
    "x_train, x_test, train_label, test_label = train_test_split(all_data[:],\n",
    "                      all_label[:], test_size=0.1, stratify=all_label[:])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "train_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=128)\n",
    "test_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=128)\n",
    "predict_encoding = tokenizer(predict_data, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/_cy/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ReviewDataset(train_encoding, train_label)\n",
    "test_dataset = ReviewDataset(test_encoding, test_label)\n",
    "predict_dataset = ReviewDataset(predict_encoding, predict_index)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "predict_dataloader = DataLoader(predict_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optim = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 1\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        if (iter_num % 10 == 0):\n",
    "            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (\n",
    "            epoch, iter_num, loss.item(), iter_num / total_iter * 100))\n",
    "\n",
    "    print(\"Epoch: %d, Average training loss: %.4f\" % (epoch, total_train_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n",
    "    print(\"Average testing loss: %.4f\" % (total_eval_loss / len(test_dataloader)))\n",
    "    print(\"-------------------------------\")\n",
    "    return avg_val_accuracy, total_eval_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0 ----------------\n",
      "epoth: 0, iter_num: 10, loss: 0.2965, 5.92%\n",
      "epoth: 0, iter_num: 20, loss: 0.2008, 11.83%\n",
      "epoth: 0, iter_num: 30, loss: 0.3289, 17.75%\n",
      "epoth: 0, iter_num: 40, loss: 0.0974, 23.67%\n",
      "epoth: 0, iter_num: 50, loss: 0.2847, 29.59%\n",
      "epoth: 0, iter_num: 60, loss: 0.2963, 35.50%\n",
      "epoth: 0, iter_num: 70, loss: 0.0263, 41.42%\n",
      "epoth: 0, iter_num: 80, loss: 0.0181, 47.34%\n",
      "epoth: 0, iter_num: 90, loss: 0.2817, 53.25%\n",
      "epoth: 0, iter_num: 100, loss: 0.1583, 59.17%\n",
      "epoth: 0, iter_num: 110, loss: 0.0137, 65.09%\n",
      "epoth: 0, iter_num: 120, loss: 0.0235, 71.01%\n",
      "epoth: 0, iter_num: 130, loss: 0.0123, 76.92%\n",
      "epoth: 0, iter_num: 140, loss: 0.2523, 82.84%\n",
      "epoth: 0, iter_num: 150, loss: 0.2606, 88.76%\n",
      "epoth: 0, iter_num: 160, loss: 0.0208, 94.67%\n",
      "Epoch: 0, Average training loss: 0.2080\n",
      "Accuracy: 0.9474\n",
      "Average testing loss: 0.1888\n",
      "-------------------------------\n",
      "------------Epoch: 1 ----------------\n",
      "epoth: 1, iter_num: 10, loss: 0.4386, 5.92%\n",
      "epoth: 1, iter_num: 20, loss: 0.0139, 11.83%\n",
      "epoth: 1, iter_num: 30, loss: 0.0134, 17.75%\n",
      "epoth: 1, iter_num: 40, loss: 0.0156, 23.67%\n",
      "epoth: 1, iter_num: 50, loss: 0.0124, 29.59%\n",
      "epoth: 1, iter_num: 60, loss: 0.2567, 35.50%\n",
      "epoth: 1, iter_num: 70, loss: 0.0143, 41.42%\n",
      "epoth: 1, iter_num: 80, loss: 0.0141, 47.34%\n",
      "epoth: 1, iter_num: 90, loss: 0.1818, 53.25%\n",
      "epoth: 1, iter_num: 100, loss: 0.2842, 59.17%\n",
      "epoth: 1, iter_num: 110, loss: 0.0376, 65.09%\n",
      "epoth: 1, iter_num: 120, loss: 0.0131, 71.01%\n",
      "epoth: 1, iter_num: 130, loss: 0.3091, 76.92%\n",
      "epoth: 1, iter_num: 140, loss: 0.0154, 82.84%\n",
      "epoth: 1, iter_num: 150, loss: 0.0144, 88.76%\n",
      "epoth: 1, iter_num: 160, loss: 0.1625, 94.67%\n",
      "Epoch: 1, Average training loss: 0.1232\n",
      "Accuracy: 0.9463\n",
      "Average testing loss: 0.1927\n",
      "-------------------------------\n",
      "------------Epoch: 2 ----------------\n",
      "epoth: 2, iter_num: 10, loss: 0.2423, 5.92%\n",
      "epoth: 2, iter_num: 20, loss: 0.0127, 11.83%\n",
      "epoth: 2, iter_num: 30, loss: 0.0143, 17.75%\n",
      "epoth: 2, iter_num: 40, loss: 0.0167, 23.67%\n",
      "epoth: 2, iter_num: 50, loss: 0.0127, 29.59%\n",
      "epoth: 2, iter_num: 60, loss: 0.0141, 35.50%\n",
      "epoth: 2, iter_num: 70, loss: 0.0196, 41.42%\n",
      "epoth: 2, iter_num: 80, loss: 0.3975, 47.34%\n",
      "epoth: 2, iter_num: 90, loss: 0.4694, 53.25%\n",
      "epoth: 2, iter_num: 100, loss: 0.0161, 59.17%\n",
      "epoth: 2, iter_num: 110, loss: 0.0145, 65.09%\n",
      "epoth: 2, iter_num: 120, loss: 0.3080, 71.01%\n",
      "epoth: 2, iter_num: 130, loss: 0.0275, 76.92%\n",
      "epoth: 2, iter_num: 140, loss: 0.2857, 82.84%\n",
      "epoth: 2, iter_num: 150, loss: 0.2480, 88.76%\n",
      "epoth: 2, iter_num: 160, loss: 0.0286, 94.67%\n",
      "Epoch: 2, Average training loss: 0.1251\n",
      "Accuracy: 0.9463\n",
      "Average testing loss: 0.1906\n",
      "-------------------------------\n",
      "------------Epoch: 3 ----------------\n",
      "epoth: 3, iter_num: 10, loss: 0.1527, 5.92%\n",
      "epoth: 3, iter_num: 20, loss: 0.0120, 11.83%\n",
      "epoth: 3, iter_num: 30, loss: 0.0145, 17.75%\n",
      "epoth: 3, iter_num: 40, loss: 0.0119, 23.67%\n",
      "epoth: 3, iter_num: 50, loss: 0.2511, 29.59%\n",
      "epoth: 3, iter_num: 60, loss: 0.5098, 35.50%\n",
      "epoth: 3, iter_num: 70, loss: 0.0150, 41.42%\n",
      "epoth: 3, iter_num: 80, loss: 0.0167, 47.34%\n",
      "epoth: 3, iter_num: 90, loss: 0.0361, 53.25%\n",
      "epoth: 3, iter_num: 100, loss: 0.0153, 59.17%\n",
      "epoth: 3, iter_num: 110, loss: 0.2266, 65.09%\n",
      "epoth: 3, iter_num: 120, loss: 0.0181, 71.01%\n",
      "epoth: 3, iter_num: 130, loss: 0.4838, 76.92%\n",
      "epoth: 3, iter_num: 140, loss: 0.3104, 82.84%\n",
      "epoth: 3, iter_num: 150, loss: 0.0206, 88.76%\n",
      "epoth: 3, iter_num: 160, loss: 0.2584, 94.67%\n",
      "Epoch: 3, Average training loss: 0.1201\n",
      "Accuracy: 0.9474\n",
      "Average testing loss: 0.1888\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "valid_loss = []\n",
    "best_loss = 100000.0\n",
    "best_epoch = 0\n",
    "ckpts_path = './ckpts'\n",
    "if not os.path.exists(ckpts_path):\n",
    "    os.makedirs(ckpts_path)\n",
    "for epoch in range(4):\n",
    "    print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "    train()\n",
    "    val_acc, val_loss = validation()\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "    torch.save(model, ckpts_path + '/' + 'model-{}.pth'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(ckpts_path + '/' + 'model-{}.pth'.format(best_epoch))\n",
    "model.eval()\n",
    "index_list = []\n",
    "pred_result_list = []\n",
    "label_dict = {1: 'pos', 0: 'neg'}\n",
    "for batch in predict_dataloader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    indexies = batch['labels']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    indexies = indexies.to('cpu').numpy().tolist()\n",
    "    preds_flat = np.argmax(logits, axis=1).flatten().tolist()\n",
    "    index_list = index_list + indexies\n",
    "    pred_result_list = pred_result_list + [label_dict[p] for p in preds_flat]\n",
    "\n",
    "with open('predict_results.txt', 'w') as f:\n",
    "    for inx, pred in zip(index_list, pred_result_list):\n",
    "        f.write(str(inx) + '\\t' + pred + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "010e35be6881e214f30871c9b30ebf5c487a5e00635ee6ec8f811c41b9fee0a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
